---
title: "Problem Set #4"
author: "Anaya Hall & Christian Miller"
date: "Due April 25th"
output: pdf_document
fontsize: 11pt
geometry: margin=.75in 
---

```{r setup, include=FALSE}

rm(list = ls())
# Setup
knitr::opts_chunk$set(echo = TRUE, cache = F)
# Options
options(stringsAsFactors = F)
# Packages
library(pacman)
p_load(knitr, kableExtra, tidyverse, dplyr, readr, magrittr, ggplot2, readxl, ascii, sandwich, tinytex)

```

# Serial Correlation
The goal of this problem set is to explore what happens when we have _serially correlated distrubances_.


## Question 1:
**Read the data into R. Plot the series against time and make sure your data are read in correctly.**
Also, print out data as ascii file and compare the first and last row to make sure there's no funny business with how the data were read in. Check a few points in the middle too.

```{r read_data, message = FALSE}

# Column names from codebook
names <- c("Year", "Qtr", "Realgdp", "Realcons", "Realinvs", "Realgovt", "Realdpi", "CPI_U", "M1", "Tbilrate", "Unemp", "Pop", "Infl", "Realint")

# Read in txt file as data.frame using column names from codebook
gdp_data <- readr::read_table2("data.txt",
                             col_names = names) 

```


``` {r plot_series, message = FALSE}

#Plot the variables in our model against time
ggplot(data = gather(gdp_data, key, value, -Year), aes(x = Year, y = value)) +
  geom_line() +
  facet_wrap(~ key, scales = "free") +
  ggtitle("GDP data variables over time") +
  ylab("Value") +
  xlab("Year") + theme_minimal()

```

```{r check_ascii}

write.table(x = gdp_data, file = "test_ascii")

# ascii(x = gdp_data, include.rownames = T)

```
So far, everthing looks good.


## Question 2: Phillips Curve
Estimate the estimations augmented Phillips Curve (see Greene p. 251)

Equation:

$\Delta p_t - \Delta p_{t-1} = \beta_1 + \beta_2\cdot u_t + \epsilon_t$

### (a) Generate dependent variable
_Hint: Check the codebook; may need to drop one of our variables._

Need to drop the first row because the first observation for Infl is missing
Phillip's curve regresses inflation (%) on unemployment (%)

```{r prepare_data}

# Generate Dependent Variable
for (i in 1:nrow(gdp_data)) {
                       if (i==1)
                       gdp_data$delta_p[i] = NA
                       else
                         gdp_data$delta_p[i] = gdp_data$Infl[i] - gdp_data$Infl[i-1] }

# Drop first observation (row)
gdp_data <- gdp_data[-1,]

```

### (b)
Estimate relationship above. Report parameter estimates, standard errors, t-statistics and $R^2$.

First, let's load our OLS function. 
```{r OLS function}

# Function to convert tibble, data.frame, or tbl_df to matrix
to_matrix <- function(the_df, vars) {
  # Create a matrix from variables in var
  new_mat <- the_df %>%
    #Select the columns given in 'vars'
    select_(.dots = vars) %>%
    # Convert to matrix
    as.matrix()
  # Return 'new_mat'
  return(new_mat)
}

ols <- function(data, y_data, X_data, intercept = T, H0 = 0, two_tail = T, alpha = 0.05) {
  # Function setup ----
    # Require the 'dplyr' package
    require(dplyr)
  
  # Create dependent and independent variable matrices ----
    # y matrix
    y <- to_matrix (the_df = data, vars = y_data)
    # X matrix
    X <- to_matrix (the_df = data, vars = X_data)
      # If 'intercept' is TRUE, then add a column of ones
      if (intercept == T) {
      X <- cbind(1,X)
      colnames(X) <- c("intercept", X_data)
      }
 
  # Calculate b, y_hat, and residuals ----
    b <- solve(t(X) %*% X) %*% t(X) %*% y
    y_hat <- X %*% b
    e <- y - y_hat
    
  # Useful -----
    n <- nrow(X) # number of observations
    k <- ncol(X) # number of independent variables
    dof <- n - k # degrees of freedom
    i <- rep(1,n) # column of ones for demeaning matrix
    A <- diag(i) - (1 / n) * i %*% t(i) # demeaning matrix
    y_star <- A %*% y # for SST
    X_star <- A %*% X # for SSM
    SST <- drop(t(y_star) %*% y_star)
    SSM <- drop(t(b) %*% t(X_star) %*% X_star %*% b)
    SSR <- drop(t(e) %*% e)
  
  # Measures of fit and estimated variance ----
    R2uc <- drop((t(y_hat) %*% y_hat)/(t(y) %*% y)) # Uncentered R^2
    R2 <- 1 - SSR/SST # Uncentered R^2
    R2adj <- 1 - (n-1)/dof * (1 - R2) # Adjusted R^2
    AIC <- log(SSR/n) + 2*k/n # AIC
    SIC <- log(SSR/n) + k/n*log(n) # SIC
    s2 <- SSR/dof # s^2
  
  # Measures of fit table ----
    mof_table_df <- data.frame(R2uc, R2, R2adj, SIC, AIC, SSR, s2)
    mof_table_col_names <- c("$R^2_\\text{uc}$", "$R^2$",
                             "$R^2_\\text{adj}$",
                             "SIC", "AIC", "SSR", "$s^2$")
    mof_table <-  mof_table_df %>% knitr::kable(
      row.names = F,
      col.names = mof_table_col_names,
      format.args = list(scientific = F, digits = 4),
      booktabs = T,
      escape = F
    )
  
  # t-test----
    # Standard error
    se <- as.vector(sqrt(s2 * diag(solve(t(X) %*% X))))
    # Vector of _t_ statistics
    t_stats <- (b - H0) / se
    # Calculate the p-values
    if (two_tail == T) {
    p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F) * 2
    } else {
      p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F)
    }
    # Do we (fail to) reject?
    reject <- ifelse(p_values < alpha, reject <- "Reject", reject <- "Fail to Reject")
    
    # Nice table (data.frame) of results
    ttest_df <- data.frame(
      # The rows have the coef. names
      effect = rownames(b),
      # Estimated coefficients
      coef = as.vector(b) %>% round(3),
      # Standard errors
      std_error = as.vector(se) %>% round(4),
      # t statistics
      t_stat = as.vector(t_stats) %>% round(3),
      # p-values
      p_value = as.vector(p_values) %>% round(4),
      # reject null?
      significance = as.character(reject)
      )
  
    ttest_table <-  ttest_df %>% knitr::kable(
      col.names = c("", "Coef.", "S.E.", "t Stat", "p-Value", "Decision"),
      booktabs = T,
      format.args = list(scientific = F),
      escape = F,
      caption = "OLS Results"
    )

  # Data frame for exporting for y, y_hat, X, and e vectors ----
    export_df <- data.frame(y, y_hat, e, X) %>% tbl_df()
    colnames(export_df) <- c("y","y_hat","e",colnames(X))
  
  # Return ----
    return(list(n=n, dof=dof, b=b, vars=export_df, R2uc=R2uc,R2=R2,
                R2adj=R2adj, AIC=AIC, SIC=SIC, s2=s2, SST=SST, SSR=SSR,
                mof_table=mof_table, ttest=ttest_table))
}

```
\newpage


``` {r model_1}

# What is the right model here??--- all covariates?
covariates <- c("Year", "Qtr", "Realgdp", "Realcons", "Realinvs", "Realgovt", "Realdpi", "CPI_U", "M1", "Tbilrate", "Unemp", "Pop", "Infl", "Realint")

model_1 <- ols(gdp_data, 
               y_data = "delta_p", 
               X_data =c("Year", "Realgdp", "Realcons", "Realinvs", "Realgovt", "Realdpi", "CPI_U", "M1", "Tbilrate", "Unemp", "Pop", "Infl", "Realint"))

model_1$ttest

model_1$mof_table

```


### (c)
Plot residuals against time

```{r plot_resid}
res <- model_1$vars$e

plot(gdp_data$Year, res)

```
Looking at this plot, we likely have a POSITVE auto-correlation issue....


### (d)



**
# -FUNCTIONS-

##### White's Test
Regress the squared residuals ($e^2_i$) on a constant, all variables in *$X$*, squares of all variables in *$X$* and all cross products. $n \dot R^2$ from this regression is distributed as a $\chi^2_{(p-1)}$, where p is the number of regressors in this equation including the constant. The null in this test is homoskedastic disturbances.


``` {r white_test_fxn, include = F}

white_test <- function(resid, cov_mat){
  
  cov_mat %<>% as.matrix()
  
  # Interaction matrix
  cov_n <- nrow(cov_mat)
  cov_k <- sum(seq(ncol(cov_mat)))
  int_mat <- matrix(NA, nrow = cov_n, ncol = cov_k)
  
  # Loop through all columns to create interaction matrix
  for (i in 1:ncol(cov_mat)) {
    for (j in i:ncol(cov_mat)) {
      if (i == 1) m <- j
      if (i > 1) m <- sum(seq(ncol(cov_mat), 1, -1)[1:(i-1)]) + (j - i + 1)
      int_mat[, m] <- cov_mat[, i] * cov_mat[, j]
    }
  }
  
  # Bind together with covariate matrix
  cov_mat %<>% cbind(.,int_mat)
  # Make sure unique (see documentation for MARGIN = 2)
  cov_mat %<>% unique(MARGIN = 2)
  # Add intercept (column of ones)
  cov_mat %<>% cbind(1,.)

  # Outcome var ('y') is squared residual
  y_data <- resid^2
  
  # y-hat for residual regression = X*beta  
  y_hat <- cov_mat %*% solve(t(cov_mat) %*% cov_mat) %*% t(cov_mat) %*% y_data 
  
  # Calculate SSM and SST for R^2
  SSM <- sum((y_hat - mean(y_data))^2)
  SST <- sum((y_data - mean(y_data))^2)
  
  # Calculate White test statistic = R^2 * n
  test_stat <- SSM / SST * cov_n
  # Calculate pvalue
  pvalue <- 1 - pchisq(test_stat, df = (ncol(cov_mat)-1)) #dof is p-1 yes?
  
  
  return(list(PValue = pvalue, TestStat = test_stat, dof=ncol(cov_mat)))
  
  # white test results
  # whitetest_df <- data.frame()
  # 
  # 
  # white_table <-  whitetest_df %>% knitr::kable(
  #     booktabs = T,
  #     format.args = list(scientific = F),
  #     escape = F,
  #     caption = "White Test")
  
}


```



##### Goldfeld-Quant Test:
``` {r goldfeldquant_fxn}

GQ_test <- function(e1, e2, k) {
  n1 <- length(e1)
  n2 <- length(e2)
  
  SSE1 <- (t(e1) %*% e1)/(n1-k)
  SSE2 <- (t(e2) %*% e2)/(n2-k)
  test_stat <- SSE1 / SSE2
  
  pvalue <- 1 - pf(test_stat, n1-k, n2-k)
  
  return(data.frame(
    "Test Statistic" = test_stat,
    "P-Value" = pvalue
  ))
}

```

```{r 2c, include = FALSE}
# Prep for test
# Rank by tenure
wage_df %<>% arrange(tenure)
# Splitting the data, removing the middle 235 observations
wage_df1 <- wage_df[1:350,]
wage_df2 <- wage_df[586:935,]
# Run two regressions saving their residuals
gq_resid_1 <- ols(wage_df1, y_data = "lwage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))$vars$e
gq_resid_2 <- ols(wage_df2, y_data = "lwage", 
               X_data = c("exper", "tenure", "married", "south", "urban", "black", "educ"))$vars$e

# Run GQ Test
GQ_test(gq_resid_1, gq_resid_2, k=7) %>% knitr::kable()

```

##### Breusch-Pagan Test for heteroskedastic errors
```{r breushpagan_fxn}
BP_test <- function(data, e, cov_mat) {
  
  df <- data
  
  n <- length(e)
  
  dep_var <- (n * e^2 / sum(e^2))
  
  bp_df <- data.frame(dep_var, cov_mat)
  
  bp_Xs <- names(bp_df[2:ncol(bp_df)])
  # run ols & save residuals
  bp_ols <- ols(bp_df, y = "dep_var", X = bp_Xs)
  yhat <- bp_ols$vars$y_hat
  
  ESS <- sum((dep_var - yhat)^2)
  
  test_stat <- (1/2)*ESS
  
  pvalue <- 1 - pchisq(test_stat, df = nrow(cov_mat))
  
  return(data.frame(
    "Test Statistic" = test_stat,
    "P-Value" = pvalue
  ))
}

```


##### White Robust Standard Errors
```{r Spherical varcov fxn}
# Function for OLS coefficient estimates
b_ols <- function(y, X) {
  # Calculate beta hat
  beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
  # Return beta_hat
  return(beta_hat)
}

# Function for OLS coef., SE, t-stat, and p-value
vcov_ols <- function(data, y_var, X_vars, intercept = T) {
  # Turn data into matrices
  y <- to_matrix(data, y_var)
  X <- to_matrix(data, X_vars)
  # Add intercept
  if (intercept == T) X <- cbind(1, X)
  # Calculate n and k for degrees of freedom
  n <- nrow(X)
  k <- ncol(X)
  # Estimate coefficients
  b <- b_ols(y, X)
  # Update names
  if (intercept == T) rownames(b)[1] <- "Intercept"
  # Calculate OLS residuals
  e <- y - X %*% b
  # Calculate s^2
  s2 <- (t(e) %*% e) / (n-k)
  s2 %<>% as.numeric()
  # Inverse of X'X
  XX_inv <- solve(t(X) %*% X)
  # Return the results
  return(as.numeric(s2) * XX_inv)
}  
```

```{r Robust varcov fxn}
vcov_white <- function(data, y_var, X_vars, intercept = T) {
  # Turn data into matrices
  y <- to_matrix(data, y_var)
  X <- to_matrix(data, X_vars)
  # Add intercept
  if (intercept == T) X <- cbind(1, X)
  # Calculate n and k for degrees of freedom
  n <- nrow(X)
  k <- ncol(X)
  # Estimate coefficients
  b <- b_ols(y, X)
  # Update names
  if (intercept == T) rownames(b)[1] <- "Intercept"
  # Calculate OLS residuals
  e <- y - X %*% b
  # Inverse of X'X
  XX_inv <- solve(t(X) %*% X)
  # For each row, calculate x_i' x_i e_i^2; then sum
  sigma_hat <- lapply(X = 1:n, FUN = function(i) {
    # Define x_i
    x_i <- matrix(as.vector(X[i,]), nrow = 1)
    # Return x_i' x_i e_i^2
    return(t(x_i) %*% x_i * e[i]^2)
  }) %>% Reduce(f = "+", x = .)
  # Return the results
  return(XX_inv %*% sigma_hat %*% XX_inv)
}
```


##### FGLS
```{r fglsfunctionattempt, include=F}
# ATTEMPT TO MAKE A FUNCTION --- do not include!
# fgls <- function(data, y_data, X_data) {
#   
#   y <- to_matrix(data, y_data)
#   X <- to_matrix(data, X_data) %>% cbind(1,.)
#   Z <- to_matrix(data, X_data) %>% cbind(1,.)
#   
#   # run first regression & save resid
#   e <- ols(y, X)$vars$e
#   
#   w <- ols(e^2, Z)$vars$y_hat
#   
#   W <- (1/sqrt(w))
#   
#   X_tilde <- X %*% X
#   y_tilde <- y %*% W
#   
#   fgls_df <- cbind(X_tilde, y_tilde)
#   
#   fgls_results <- ols(fgls_df, colnames(tilde)[1], colnames(tilde)[2:ncol(tilde)], intercept = F)
#   
#   return(fgls_results)
# }
# 
# #run function on data
# 
# fgls(wage_df, wage_df$lwage, cov_mat)
```

